{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd1e6cdc-5306-4d1e-a39b-a5fe1ab9ebc5",
   "metadata": {},
   "source": [
    "# Model Report: Linear Regression and LSTM with Self-Attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6e51e4-26f7-41f8-8823-bbab395c9c05",
   "metadata": {},
   "source": [
    "In this report, we will discuss two powerful machine learning models used for predicting time-series data: Linear Regression and LSTM (Long Short-Term Memory) with an integrated Self-Attention mechanism. These models are commonly applied in forecasting, financial prediction, and other domains that require understanding and predicting sequential data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdbaee1-2b3e-4125-8e8f-9024545faadd",
   "metadata": {},
   "source": [
    "## 1. Linear Regression Model\n",
    "### 1.1 Overview\n",
    "\n",
    "Linear regression is a widely-used statistical technique that models the relationship between a dependent variable \n",
    "ùë¶\n",
    " and one or more independent variables \n",
    "ùëã\n",
    " by fitting a linear equation to the observed data. It assumes that the relationship between the variables can be approximated by a straight line.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c295f0-4e35-46c8-8beb-81b84ff2766a",
   "metadata": {},
   "source": [
    "### 1.2 Mathematical Formula\n",
    "The linear regression model can be represented as:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n + \\epsilon\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ y $ is the dependent variable.\n",
    "- $ X_1, X_2, \\dots, X_n $ are the independent variables (features).\n",
    "- $ \\beta_0 $ is the intercept (bias term).\n",
    "- $ \\beta_1, \\dots, \\beta_n $ are the coefficients (weights) for each feature.\n",
    "- $ \\epsilon $ is the error term.\n",
    "\n",
    "The model is trained by minimizing the **Mean Squared Error (MSE)**:\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ y_i $ is the true value of the dependent variable.\n",
    "- $ \\hat{y}_i $ is the predicted value of the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b86df7c-23a3-4568-833c-64486c138666",
   "metadata": {},
   "source": [
    "### 1.3 Model Implementation\n",
    "In Python, we implement the Linear Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a76fb3-9b58-4f61-826b-2ab4d592554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def Regression_model():\n",
    "    model = LinearRegression()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76213380-5b30-4615-b43c-74556468d3e9",
   "metadata": {},
   "source": [
    "1.4 Training and Optimization\n",
    "The model is trained by minimizing the **Mean Squared Error (MSE)**:\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ y_i $ is the true value of the dependent variable.\n",
    "- $\\hat{y}_i $ is the predicted value of the dependent variable.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c42612-7561-4060-b92e-3f75a15e8297",
   "metadata": {},
   "source": [
    "# 2. LSTM Model with Self-Attention\n",
    "### 2.1 Overview\n",
    "Long Short-Term Memory (LSTM) networks are a type of recurrent neural network (RNN) designed to handle sequential data. LSTM networks have the ability to learn and retain long-term dependencies in sequential data, making them well-suited for time-series forecasting, language modeling, and other sequence prediction tasks.\n",
    "\n",
    "The addition of Self-Attention mechanisms enables the model to focus on important parts of the input sequence, improving its ability to learn relationships across different time steps.\n",
    "\n",
    "### 2.2 Mathematical Formulation\n",
    "#### 2.2.1 LSTM Cell\n",
    "The LSTM cell is defined by the following equations that regulate the flow of information through its gates:\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
    "$$\n",
    "$$\n",
    "i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
    "$$\n",
    "$$\n",
    "\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)\n",
    "$$\n",
    "$$\n",
    "C_t = f_t \\cdot C_{t-1} + i_t \\cdot \\tilde{C}_t\n",
    "$$\n",
    "$$\n",
    "o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
    "$$\n",
    "$$\n",
    "h_t = o_t \\cdot \\tanh(C_t)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ f_t $ is the forget gate.\n",
    "- $ i_t $ is the input gate.\n",
    "- $ \\tilde{C}_t $ is the candidate memory cell.\n",
    "- $ C_t $ is the memory cell state.\n",
    "- $ o_t $ is the output gate.\n",
    "- $ h_t $ is the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9da84d5-5be4-4c7d-9b12-6310d57adcdf",
   "metadata": {},
   "source": [
    "#### Self-Attention\n",
    "\n",
    "The Self-Attention mechanism computes attention scores for each input token, allowing the model to focus on different parts of the sequence at each time step. The attention mechanism is defined as:\n",
    "\n",
    "$$\n",
    "Q = XW_Q, \\quad K = XW_K, \\quad V = XW_V\n",
    "$$\n",
    "$$\n",
    "\\text{Attention Scores} = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)\n",
    "$$\n",
    "$$\n",
    "\\text{Output} = \\text{Attention Scores} \\cdot V\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ Q $, $ K $, and $ V $ represent queries, keys, and values, respectively.\n",
    "- $ d_k $ is the dimension of the key vector.\n",
    "- The softmax function ensures that the attention scores sum to 1, making them interpretable as probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22006d74-028b-469f-b5b0-ae785b192d47",
   "metadata": {},
   "source": [
    "## 2.3 Model Architecture\n",
    "The model consists of an LSTM layer followed by a Self-Attention layer and a Dense output layer. The LSTM layer captures temporal dependencies, while the Self-Attention layer allows the model to focus on relevant parts of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deecce84-6271-4917-a0ce-0350e62e3b08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Flatten\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "\n",
    "# Self Attention Layer\n",
    "class SelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.W_q = self.add_weight(shape=(input_shape[-1], input_shape[-1]), initializer=GlorotUniform(), trainable=True)\n",
    "        self.W_k = self.add_weight(shape=(input_shape[-1], input_shape[-1]), initializer=GlorotUniform(), trainable=True)\n",
    "        self.W_v = self.add_weight(shape=(input_shape[-1], input_shape[-1]), initializer=GlorotUniform(), trainable=True)\n",
    "        super(SelfAttention, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        Q = tf.matmul(inputs, self.W_q)\n",
    "        K = tf.matmul(inputs, self.W_k)\n",
    "        V = tf.matmul(inputs, self.W_v)\n",
    "        attention_scores = tf.nn.softmax(tf.matmul(Q, K, transpose_b=True) / tf.sqrt(tf.cast(K.shape[-1], tf.float32)))\n",
    "        return tf.matmul(attention_scores, V)\n",
    "\n",
    "# Model construction\n",
    "def build_LSTM(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    lstm_out = LSTM(50, return_sequences=True)(input_layer)\n",
    "    attention_out = SelfAttention()(lstm_out)\n",
    "    flatten_out = Flatten()(attention_out)\n",
    "\n",
    "    output_layer = Dense(1, activation='linear')(flatten_out)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13fc5fd-5b50-4f5f-a80e-d7d8a04259d8",
   "metadata": {},
   "source": [
    "### 2.4 Training and Optimization\n",
    "The LSTM model is trained to minimize the Mean Squared Error (MSE), which is the same loss function used in the regression model. The optimizer used is Adam, a popular gradient-based optimizer known for its adaptive learning rate and efficiency in training deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c2dd54-d5ea-442d-ad8b-aeaf5c150ca2",
   "metadata": {},
   "source": [
    "## 3. Conclusion\n",
    "In this report, we have discussed two models: the Linear Regression model and the LSTM model with Self-Attention.\n",
    "\n",
    "Linear Regression provides a simple and interpretable model for predicting continuous variables.\n",
    "LSTM with Self-Attention offers an advanced method for time-series forecasting by capturing long-term dependencies and allowing the model to focus on relevant information through attention mechanisms.\n",
    "Both models are useful for different tasks depending on the complexity of the data and the problem at hand. The Linear Regression model is a great starting point for problems with linear relationships, while the LSTM model is more suited for sequential or time-series data that requires capturing complex temporal dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a71ee2c-3509-42c5-a602-11c90906f948",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
